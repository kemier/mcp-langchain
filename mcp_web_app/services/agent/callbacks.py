#!/usr/bin/env python3
"""
Callback handlers for streaming and event handling in agents.
"""
import logging
from typing import Any, Dict, Optional, List, Union

from langchain_core.callbacks.base import BaseCallbackHandler
from langchain_core.outputs import LLMResult
from langchain_core.messages import AIMessage, BaseMessage

from ...utils.custom_event_handler import EventType

# Initialize logger
logger = logging.getLogger(__name__)

class StreamingCallback(BaseCallbackHandler):
    """
    A simple callback handler for streaming tokens from an LLM.
    
    This handler forwards tokens to a provided output function as they
    are generated, enabling real-time streaming of LLM outputs.
    """
    
    def __init__(self, output_stream_fn):
        """
        Initialize the streaming callback.
        
        Args:
            output_stream_fn: A function that takes (event_type, data) and
                             handles streaming the event to the client
        """
        super().__init__()
        self.output_stream_fn = output_stream_fn
    
    async def on_llm_new_token(self, token: str, **kwargs: Any) -> None:
        """
        Handle new tokens as they are generated.
        
        Args:
            token: The token generated by the LLM
            kwargs: Additional arguments
        """
        logger.debug(f"StreamingCallback on_llm_new_token: '{token}'")
        await self.output_stream_fn(EventType.TOKEN, token)
    
    async def on_llm_end(self, response: LLMResult, **kwargs: Any) -> None:
        """
        Handle LLM generation completion.
        
        Args:
            response: The LLM result
            kwargs: Additional arguments
        """
        logger.debug(f"StreamingCallback on_llm_end. Response: {response}")
        # No specific action needed here as we stream tokens incrementally
    
    async def on_llm_error(self, error: BaseException, **kwargs: Any) -> None:
        """
        Handle LLM errors.
        
        Args:
            error: The error that occurred
            kwargs: Additional arguments
        """
        logger.error(f"StreamingCallback on_llm_error: {error}", exc_info=True)
        await self.output_stream_fn(EventType.ERROR, {"error": str(error)})
    
    # For future enhancements, may implement more callback methods:
    # on_chat_model_start, on_tool_start, on_tool_end, on_agent_action, etc.

class MCPEventCollector(BaseCallbackHandler):
    """
    Callback handler that collects the final output from an agent execution.
    
    This is useful when streaming via astream_events, as it can be difficult
    to extract the final response from the event stream directly.
    """
    
    def __init__(self):
        """Initialize the event collector."""
        super().__init__()
        self._final_output = None
        self._collected_agent_actions = []
        self._collected_tool_outputs = []
        self._raw_events = []
        self._full_message = None
        self._chain_end_outputs = []
        
    def get_final_output(self) -> Optional[str]:
        """
        Get the final output collected during execution.
        
        Returns:
            The final output string, or None if no output was collected
        """
        # First try the full message if available
        if self._full_message:
            return self._full_message
            
        # Next try the last chain_end output if available
        if self._chain_end_outputs:
            return self._chain_end_outputs[-1]
            
        # Fall back to the explicit final output
        return self._final_output
    
    def get_agent_actions(self) -> List[Dict[str, Any]]:
        """
        Get the agent actions collected during execution.
        
        Returns:
            List of agent actions
        """
        return self._collected_agent_actions
    
    def get_tool_outputs(self) -> List[Dict[str, Any]]:
        """
        Get the tool outputs collected during execution.
        
        Returns:
            List of tool outputs
        """
        return self._collected_tool_outputs
    
    def on_agent_finish(self, finish: Any, **kwargs: Any) -> None:
        """
        Handle agent finish event.
        
        Args:
            finish: The agent finish data
            kwargs: Additional arguments
        """
        if hasattr(finish, "return_values") and "output" in finish.return_values:
            self._final_output = finish.return_values["output"]
            logger.debug(f"MCPEventCollector: Captured final output on_agent_finish: {self._final_output}")
    
    def on_chain_end(self, outputs: Dict[str, Any], **kwargs: Any) -> None:
        """
        Handle chain end event.
        
        Args:
            outputs: The chain outputs
            kwargs: Additional arguments
        """
        # Capture chain output, especially for RunnableWithMessageHistory
        extracted_output = None
        
        if isinstance(outputs, dict):
            # First check for a direct 'output' key
            if "output" in outputs:
                extracted_output = outputs["output"]
            # Then check for nested content or AIMessage
            elif "response" in outputs and hasattr(outputs["response"], "content"):
                extracted_output = outputs["response"].content
        
        # If outputs is an AIMessage or has content attribute
        elif hasattr(outputs, "content"):
            extracted_output = outputs.content
        
        # Store the extracted output if found
        if extracted_output:
            self._chain_end_outputs.append(extracted_output)
            logger.debug(f"MCPEventCollector: Captured chain end output: {extracted_output[:100]}")
    
    def on_agent_action(self, action: Any, **kwargs: Any) -> None:
        """
        Handle agent action event.
        
        Args:
            action: The agent action
            kwargs: Additional arguments
        """
        if hasattr(action, "tool") and hasattr(action, "tool_input"):
            action_record = {
                "tool": action.tool,
                "input": action.tool_input
            }
            self._collected_agent_actions.append(action_record)
            logger.debug(f"MCPEventCollector: Recorded agent action: {action.tool}")
    
    def on_tool_end(self, output: str, **kwargs: Any) -> None:
        """
        Handle tool end event.
        
        Args:
            output: The tool output
            kwargs: Additional arguments
        """
        self._collected_tool_outputs.append({"output": output})
        logger.debug(f"MCPEventCollector: Recorded tool output: {output[:100]}")
    
    def on_text(self, text: str, **kwargs: Any) -> None:
        """
        Handle text event.
        
        Args:
            text: The text
            kwargs: Additional arguments
        """
        # Sometimes the final response comes through as text
        self._full_message = text
        logger.debug(f"MCPEventCollector: Captured full message text: {text[:100]}")
    
    def on_chat_model_end(self, messages: List[BaseMessage], **kwargs: Any) -> None:
        """
        Handle chat model end event.
        
        Args:
            messages: The chat model messages
            kwargs: Additional arguments
        """
        if messages and isinstance(messages[-1], AIMessage):
            self._full_message = messages[-1].content
            logger.debug(f"MCPEventCollector: Captured chat model end message: {self._full_message[:100]}")
    
    def on_llm_end(self, response: LLMResult, **kwargs: Any) -> None:
        """
        Handle LLM end event.
        
        Args:
            response: The LLM result
            kwargs: Additional arguments
        """
        # Could extract content from LLMResult if needed
        pass 